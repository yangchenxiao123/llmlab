{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Lab 7: LLM API server and Web interfaces\n","\n","In this lecture, you will learn how to serve modern large models on Linux servers with easy-to-use user interface. We will be using Python as our main programming language, and we do not require knowledge about front-end language such as Javascript or CSS."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1 Calling Web Service APIs\n","\n","In this experiment, we'll equip you with the basic knowledge and practical skills to start making powerful HTTP requests in Python. We'll cover GET and POST methods, and explore JSON data exchange. So, buckle up, let's code!\n","\n","First, we will need `requests` library. Install it with the following command."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests) (2023.7.22)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install requests"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1 Basic `GET`\n","\n","GET retrieves information from a specific web address (URL). Parameters are passed either in the path itself or as a query parameter (after ? in the URL).\n","\n","Let's try the GET method to retrieve a random joke!"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Status code: 200\n","\n","--- Response Text ---\n","{\"categories\":[],\"created_at\":\"2020-01-05 13:42:27.496799\",\"icon_url\":\"https://assets.chucknorris.host/img/avatar/chuck-norris.png\",\"id\":\"MCqtvLI4SaumUznRg7A5BA\",\"updated_at\":\"2020-01-05 13:42:27.496799\",\"url\":\"https://api.chucknorris.io/jokes/MCqtvLI4SaumUznRg7A5BA\",\"value\":\"Chuck Norris's power level is over 9000......in his sleep.\"}\n"]}],"source":["import requests\n","\n","# Target URL\n","url = \"https://api.chucknorris.io/jokes/random\"\n","\n","# Send a GET request and store the response\n","response = requests.get(url)\n","\n","# Check the response status code (2XX means success)\n","print(f\"Status code: {response.status_code}\")\n","\n","# Access the response content (raw bytes)\n","content = response.content\n","\n","# Decode the content to text (may differ depending on API)\n","text = content.decode(response.encoding)\n","\n","# Print the response\n","print(\"\\n--- Response Text ---\")\n","print(text)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.2 Playing with JSON\n","\n","Many APIs and websites return data in the JSON format, a structured way to organize information. We can easily convert this JSON string to a Python dictionary for easy access:"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'categories': [],\n"," 'created_at': '2020-01-05 13:42:27.496799',\n"," 'icon_url': 'https://assets.chucknorris.host/img/avatar/chuck-norris.png',\n"," 'id': 'MCqtvLI4SaumUznRg7A5BA',\n"," 'updated_at': '2020-01-05 13:42:27.496799',\n"," 'url': 'https://api.chucknorris.io/jokes/MCqtvLI4SaumUznRg7A5BA',\n"," 'value': \"Chuck Norris's power level is over 9000......in his sleep.\"}\n","{\"categories\": [], \"created_at\": \"2020-01-05 13:42:27.496799\", \"icon_url\": \"https://assets.chucknorris.host/img/avatar/chuck-norris.png\", \"id\": \"MCqtvLI4SaumUznRg7A5BA\", \"updated_at\": \"2020-01-05 13:42:27.496799\", \"url\": \"https://api.chucknorris.io/jokes/MCqtvLI4SaumUznRg7A5BA\", \"value\": \"Chuck Norris's power level is over 9000......in his sleep.\"}\n"]}],"source":["import json\n","from pprint import pprint\n","\n","dict = json.loads(text)\n","pprint(dict)\n","\n","encoded_json = json.dumps(dict)\n","print(encoded_json)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.3 Moving on to POST Requests\n","\n","While GET requests fetch data, POST requests send information to a server, like submitting a form. We'll be using a dummy API that echos the data we sent as an example."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Status code: 200\n","{\n","  \"args\": {}, \n","  \"data\": \"\", \n","  \"files\": {}, \n","  \"form\": {\n","    \"age\": \"30\", \n","    \"name\": \"John Doe\"\n","  }, \n","  \"headers\": {\n","    \"Accept\": \"*/*\", \n","    \"Accept-Encoding\": \"gzip, deflate, br, zstd\", \n","    \"Content-Length\": \"20\", \n","    \"Content-Type\": \"application/x-www-form-urlencoded\", \n","    \"Host\": \"httpbin.org\", \n","    \"User-Agent\": \"python-requests/2.31.0\", \n","    \"X-Amzn-Trace-Id\": \"Root=1-66446c2a-60db626201f88d25347c63e0\"\n","  }, \n","  \"json\": null, \n","  \"method\": \"POST\", \n","  \"origin\": \"114.253.254.93\", \n","  \"url\": \"https://httpbin.org/anything\"\n","}\n","\n"]}],"source":["# Define URL and data\n","url = \"https://httpbin.org/anything\"\n","data = {\"name\": \"John Doe\", \"age\": 30}  # a python dictionary\n","\n","# Send POST request with data\n","response = requests.post(url, data=data) # data is automatically encoded to json\n","\n","# Check status code and print response\n","print(f\"Status code: {response.status_code}\")\n","print(response.text)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can see that the sent data is actually received by the server (`form` shows the exactly the same data we sent).\n","\n","This is just the tip of the iceberg! Now you have seen how we can utilize the existing web service. In the remaining experiments, you will be building your own API server and web service with a nice user interface."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2 Creating an API server using FastAPI\n","\n","Most of you should have experienced the LLM APIs we provided, which allows your program accessing the power of large language models. Here we will guide you to build your own LLM service, using the `fastapi` library of Python.\n","\n","`fastapi` takes care of the job of launching a web server and serve the API calls. You only need to define a function that takes the input data from the request to produce output. `fastapi` will handle the rest things for you.\n","\n","First, install the dependency of `fastapi`."]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Basics on FastAPI"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n","Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (0.27.1)\n","Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (0.110.0)\n","Requirement already satisfied: websockets in /opt/conda/lib/python3.10/site-packages (11.0.3)\n","Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n","Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: typing-extensions>=4.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn) (4.10.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from fastapi) (2.6.3)\n","Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /opt/conda/lib/python3.10/site-packages (from fastapi) (0.36.3)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.16.3)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.37.0,>=0.36.3->fastapi) (4.3.0)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.3.1)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi) (1.0.4)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install uvicorn fastapi websockets"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /tmp/fastapi_example.py\n"]}],"source":["%%file /tmp/fastapi_example.py\n","\n","from fastapi import FastAPI, Request\n","from pydantic import BaseModel\n","import uvicorn\n","\n","app = FastAPI()\n","\n","## path parameters\n","@app.get('/g/{data}')\n","async def process_data(data: str):\n","    return f'Processed {data} by FastAPI!'\n","\n","fake_items_db = [{\"item_name\": \"Foo\"}, {\"item_name\": \"Bar\"}, {\"item_name\": \"Baz\"}]\n","# Query parameters\n","@app.get(\"/items/\")\n","async def read_item(skip: int = 0, limit: int = 10):\n","    return fake_items_db[skip : skip + limit]\n","\n","\n","## The data model\n","from typing import List\n","class Sale(BaseModel):\n","    day: int\n","    price: float\n","    \n","class Item(BaseModel):\n","    name: str\n","    inventory: int | None = 10\n","    sales: List[Sale] = []\n","\n","# Getting Parameters from Request\n","@app.post(\"/post\")\n","async def create_item(item: Item):\n","    return f'Hello {item.name}, {item.inventory} in stock, sold {len(item.sales)} items'\n","\n","# The main() function is the entry point of the script\n","if __name__ == '__main__':\n","    uvicorn.run(app, host='0.0.0.0', port=54223, workers=1)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["## run the following command in your terminal to start the server\n","## python /tmp/fastapi_example.py "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Status code: 200\n"]},{"data":{"text/plain":["b'\"Processed hello by FastAPI!\"'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# you can visit your web service at:\n","\n","response = requests.get('http://localhost:54223/g/hello')\n","print(f\"Status code: {response.status_code}\")\n","response.content"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Status code: 200\n"]},{"data":{"text/plain":["b'[{\"item_name\":\"Baz\"}]'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# Using the query parameter\n","\n","response = requests.get('http://localhost:54223/items?skip=2&limit=3')\n","print(f\"Status code: {response.status_code}\")\n","response.content\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Now let the magic happen.\n","# Set port forwarding in your VSCode devcontainer to forward port 54223 to your local machine\n","# Then visit `http://127.0.0.1:54223/g/hello` in your browser, you will be able to see the return string in the browser!\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Status code: 200\n","\"Hello Apple, 33 in stock, sold 2 items\"\n"]}],"source":["# Also test the POST processing, with a complex data structure as input\n","\n","url = \"http://localhost:54223/post\"\n","data = { \"name\": \"Apple\", \n","         \"inventory\": 33, \n","         \"sales\": [{\"day\": 0, \"price\": 3.4}, {\"day\": 1, \"price\": 3.3}]\n","         }\n","encoded = json.dumps(data).encode(\"utf-8\")\n","response = requests.post(url, data=encoded)  # the parameters should be encoded as JSON\n","print(f\"Status code: {response.status_code}\")\n","print(response.text)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# Another FastAPI magic: automatic document generation\n","# Visit http://localhost:54223/docs in your browser to see the API documentation\n","# (Assuming that you have your port forwarding set up correctly)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Creating an API to serve local LLM model"]},{"cell_type":"markdown","metadata":{},"source":["First, let's recall how you run a local LLM.  The following scripts starts a Phi-3 model."]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /tmp/local_llm.py\n"]}],"source":["%%file /tmp/local_llm.py\n","\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","\n","def chat_resp(model, tokenizer, user_prompt=None, history=[]):\n","    pipe = pipeline(\n","        \"text-generation\",\n","        model=model,\n","        tokenizer=tokenizer,\n","    )   \n","    generation_args = {\n","        \"max_new_tokens\": 500,\n","        \"return_full_text\": False,\n","        \"temperature\": 0.6,\n","        \"do_sample\": True,\n","    }\n","    if not history:\n","        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n","    else:\n","        messages = history\n","    if user_prompt:\n","        prompt_msg = [{\"role\": \"user\", \"content\": user_prompt}]\n","        messages.extend(prompt_msg)\n","    output = pipe(messages, **generation_args)\n","    return output\n","\n","## The main function is the entry point of the script\n","if __name__ == '__main__':\n","    model_path = '/ssdshare/Phi-3-mini-128k-instruct/'\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model = AutoModelForCausalLM.from_pretrained(model_path, \n","                                             device_map=\"cuda:0\", \n","                                             torch_dtype=\"auto\", \n","                                             trust_remote_code=True)\n","    resp = chat_resp(model, tokenizer, \"What is the meaning of life?\")\n","    print(resp)\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["## first verify that you can run LLM locally correctly (it should print out the results, despite of lots of warnings.)\n","## python /tmp/local_llm.py"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /tmp/api_llm.py\n"]}],"source":["%%file /tmp/api_llm.py\n","\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","         \n","from fastapi import FastAPI, Request\n","from pydantic import BaseModel\n","import uvicorn\n","\n","from urllib.parse import unquote\n","\n","app = FastAPI()\n","\n","def chat_resp(model, tokenizer, user_prompt=None, history=[]):\n","    pipe = pipeline(\n","        \"text-generation\",\n","        model=model,\n","        tokenizer=tokenizer,\n","    )   \n","    generation_args = {\n","        \"max_new_tokens\": 500,\n","        \"return_full_text\": False,\n","        \"temperature\": 0.6,\n","        \"do_sample\": True,\n","    }\n","    if not history:\n","        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n","    else:\n","        messages = history\n","    if user_prompt:\n","        prompt_msg = [{\"role\": \"user\", \"content\": user_prompt}]\n","        messages.extend(prompt_msg)\n","    output = pipe(messages, **generation_args)\n","    return output\n","\n","#### Your Task ####\n","## Implement a GET handler that takes in a single string as prompt from user,\n","## and return the response as a single string.\n","class Prompt(BaseModel):\n","    text: str\n","\n","@app.get(\"/ask/\")\n","async def get_response(prompt: str):\n","    return chat_resp(model, tokenizer, user_prompt=prompt)\n","#### End Task ####\n","\n","#### Your Task ####\n","## Implement a POST handler that takes in a single string and a history\n","## and return the response as a single string.\n","class PH(BaseModel):\n","    prompt: str\n","    history: list\n","\n","@app.post(\"/post/\")\n","async def post_response(request: Request, ph: PH):\n","    return chat_resp(model, tokenizer, user_prompt=ph.prompt, history=ph.history)\n","\n","#### End Task ####\n","\n","#### Your Task ####\n","## The main function is the entry point of the script, you should load the model\n","## and then start the FastAPI server.\n","if __name__ == '__main__':\n","    model_path = '/ssdshare/Phi-3-mini-128k-instruct/'\n","    tokenizer = AutoTokenizer.from_pretrained(model_path)\n","    model = AutoModelForCausalLM.from_pretrained(model_path, \n","                                             device_map=\"cuda:0\", \n","                                             torch_dtype=\"auto\", \n","                                             trust_remote_code=True)\n","    uvicorn.run(app, host='0.0.0.0', port=54223, workers=1)\n","#### End Task ####\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["## run the following command in your terminal to start the server\n","## python /tmp/api_llm.py"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["http://localhost:54223/ask/?prompt=%E4%B8%AD%E5%9B%BD%E7%9A%84%E9%A6%96%E9%83%BD%E6%98%AF%E5%93%AA%E9%87%8C%EF%BC%9F\n","Status code: 200\n","[{\"generated_text\":\" 中国的首都是北京。北京是中国的政治、文化和国际交流中心，也是世界上最大的人口密集地区之一。\"}]\n","http://localhost:54223/ask/?prompt=%E4%B8%AD%E5%9B%BD%E7%9A%84%E9%83%BD%E9%A6%96%E6%98%AF%E5%93%AA%E9%87%8C%EF%BC%9F\n","Status code: 200\n","[{\"generated_text\":\" 中国的首都是北京。北京是中华人民共和国的政治、文化和国际交流中心，也是其最大的城市。北京的历史可追溯到更早的秦朝，有着超过三千年的历史。\"}]\n"]}],"source":["## Run a single query to test the API, using GET\n","import requests\n","import urllib.parse\n","params = {\"prompt\": \"中国的首都是哪里？\"}\n","prompt_url = urllib.parse.urlencode(params)\n","url = f'http://localhost:54223/ask/?%s' % prompt_url\n","print(url)\n","response = requests.get(url)\n","print(f\"Status code: {response.status_code}\")\n","print(response.content.decode(response.encoding))\n","\n","params = {\"prompt\": \"中国的都首是哪里？\"}\n","prompt_url = urllib.parse.urlencode(params)\n","url = f'http://localhost:54223/ask/?%s' % prompt_url\n","print(url)\n","response = requests.get(url)\n","print(f\"Status code: {response.status_code}\")\n","print(response.content.decode(response.encoding))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Status code: 200\n","[{'generated_text': ' 中国的都首是北京。'}]\n"]}],"source":["#### Your Task ####\n","## Run a LLM single line query with POST, and add chat history (history stored on the client side only)\n","#### Your Task ####\n","## Run a LLM single line query with POST, and add chat history (history stored on the client side only)\n","import requests\n","\n","url = 'http://localhost:54223/post/'\n","data = {\n","    \"prompt\": \"中国的都首是哪里？\",\n","    \"history\": [\n","        {\"role\": \"user\", \"content\": \"法国的都首是哪里?\"},\n","        {\"role\": \"system\", \"content\": \"是巴黎.\"},\n","        {\"role\": \"user\", \"content\": \"美国的都首是哪里?\"},\n","        {\"role\": \"system\", \"content\": \"是华盛顿.\"},\n","        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n","    ]\n","}\n","response = requests.post(url, json=data)\n","print(f\"Status code: {response.status_code}\")\n","print(response.json())\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3 Adding a Web User Interface using `gradio`\n","\n","Demo a machine learning application is important. It gives the users a direct experience of your algorithm in an interactive manner. Here we'll be building an interesting demo using `gradio`, a popular Python library for ML demos. Let's install this library."]},{"cell_type":"markdown","metadata":{},"source":["### 3.1 Basic Gradio"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n","Requirement already satisfied: gradio in /opt/conda/lib/python3.10/site-packages (4.31.2)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (23.2.1)\n","Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.3.0)\n","Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.110.0)\n","Requirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio) (0.3.2)\n","Requirement already satisfied: gradio-client==0.16.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.16.3)\n","Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.21.3)\n","Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.1.2)\n","Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\n","Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.1)\n","Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.8.3)\n","Requirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.0)\n","Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.15)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (23.2)\n","Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.1)\n","Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.0.1)\n","Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.6.3)\n","Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.9)\n","Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\n","Requirement already satisfied: ruff>=0.2.2 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.4.3)\n","Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.10.0)\n","Requirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.0)\n","Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\n","Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.10.0)\n","Requirement already satisfied: urllib3~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.1)\n","Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.16.3->gradio) (2023.10.0)\n","Requirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.16.3->gradio) (11.0.3)\n","Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n","Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n","Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2023.7.22)\n","Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.4)\n","Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.4)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.9.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.65.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.16.3)\n","Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n","Requirement already satisfied: starlette<0.37.0,>=0.36.3 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.36.3)\n","Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.0.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n","Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gradio --upgrade"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Then we are able to write an example UI that takes in a text string and output a processed string. "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /tmp/gradio_example.py\n"]}],"source":["%%file /tmp/gradio_example.py\n","\n","import gradio as gr\n","\n","def greet(name, intensity):\n","    return \"Hello, hello \" + name + \"!\" * int(intensity)\n","\n","demo = gr.Interface(\n","    fn=greet,\n","    inputs=[\"text\", \"slider\"],\n","    outputs=[\"text\"],\n",")\n","\n","demo.launch(share=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Start the gradio server by runnning the following command\n","\n","# python /tmp/gradio_example.py"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n","\n","## Try change the last line (launch) to \n","\n","## demo.launch(share=True) \n","## observe the output and see the link to open (without the need of port forwarding)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 The ChatInterfae"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /tmp/gradio_example.py\n"]}],"source":["%%file /tmp/gradio_example.py\n","\n","import random\n","\n","def random_response(message, history):\n","    return random.choice([\"Yes\", \"No\"])\n","\n","import gradio as gr\n","gr.ChatInterface(fn = random_response, server_port = 7860).launch(share=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Kill your previous process, and restart the new process\n","\n","# python /tmp/gradio_example.py\n","\n","## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n","## If you do not kill the previous one, the port number will change to 7861 automatically. "]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 Quick and dirty way of creating a UI for a HuggingFace pipeline"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /tmp/simpleui.py\n"]}],"source":["%%file /tmp/simpleui.py\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","import gradio as gr\n","\n","model_path = '/ssdshare/Phi-3-mini-128k-instruct/'\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","model = AutoModelForCausalLM.from_pretrained(model_path, \n","                                             device_map=\"cuda:0\", \n","                                             torch_dtype=\"auto\", \n","                                             trust_remote_code=True)\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    temperature=0.6,\n","    do_sample=True,\n","    return_full_text=False,\n","    max_new_tokens=500,\n",") \n","gr.Interface.from_pipeline(pipe).launch(share=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# python /tmp/simpleui.py\n","\n","## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n","## If you do not kill the previous one, the port number will change to 7861 or 7862 automatically. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.4 A better way to build a web UI for LLM (through an LLM API server)\n","\n","Next, you should implement a script that interact with the Phi-3 Chat API server you just created.  \n","\n","Note that you should directly call the API server using request, instead of running the LLM within your UI server process. \n","\n","![Illustration of request](./assets/request.jpg)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n","os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n","os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /tmp/chatUI.py\n"]}],"source":["%%file /tmp/chatUI.py\n","\n","import gradio as gr\n","import requests\n","import json\n","\n","API_SERVER_URL = \"http://localhost:54223\" # Don't forget to start your local API server\n","\n","def predict(message, history):\n","\n","#### Your Task ####\n","    Pr = {\n","        \"prompt\": message,\n","        \"history\": history.split(\"\\n\") if history else []  # Split history string into a list of strings\n","    }\n","    # Send POST request to the /post_response/ endpoint with the updated payload\n","    response = requests.post(f\"{API_SERVER_URL}/post/\", json=Pr)\n","    if response.status_code == 200:\n","        return response.json()\n","    else:\n","        return \"Error %s: Failed to get response from the server\" % response.status_code\n","#### End Task ####\n","\n","gr.ChatInterface(predict).launch(show_error=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Do not forget to start your API server (from above, with the /chat API.)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Add the port forwarding (port 7860 by default), and you can see http://localhost:7860 in your browser\n","## If you do not kill the previous one, the port number will change to 7861 or 7862 automatically. "]},{"cell_type":"markdown","metadata":{},"source":["You you can also test it programmatically using gradio-client. "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n","Requirement already satisfied: gradio-client in /opt/conda/lib/python3.10/site-packages (0.16.3)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client) (2023.10.0)\n","Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio-client) (0.27.0)\n","Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio-client) (0.21.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio-client) (23.2)\n","Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client) (4.10.0)\n","Requirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client) (11.0.3)\n","Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio-client) (4.3.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio-client) (2023.7.22)\n","Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio-client) (1.0.4)\n","Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio-client) (3.4)\n","Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio-client) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio-client) (0.14.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio-client) (3.9.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio-client) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio-client) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio-client) (6.0.1)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio-client) (1.0.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio-client) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio-client) (2.2.1)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install gradio-client"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded as API: http://127.0.0.1:7861/ ✔\n","[{'generated_text': ' Hello there! How can I assist you today?'}]\n"]}],"source":["from gradio_client import Client\n","\n","client = Client(\"http://127.0.0.1:7861/\")\n","result = client.predict(\n","\t\tmessage=\"Hello!!\",\n","\t\tapi_name=\"/chat\"\n",")\n","print(result)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5 More Gradio: create an UI to serve an image model you created in lab 5.\n","\n","You can either use the from_pipeline() or create your own, more advanced UI.  In either way, you will need to allow API access to your service (will be needed for the following labs). \n","\n","If you feel more adventurous, try a new multi-media model, such as text-to-speach or voice recognition.  We have downloaded some for you at:\n","/share/model/speecht5_hifigan/,/share/model/speecht5_tts/, and /share/model/whisper-medium/\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Keyword arguments {'use_auth_token': False} are not expected by StableDiffusionPipeline and will be ignored.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4124f6e7c9f1422691297a9f546895fe","version_major":2,"version_minor":0},"text/plain":["Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Running on local URL:  http://127.0.0.1:7860\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":1,"metadata":{},"output_type":"execute_result"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d1b481bf6f24749bd76983d54a3ce6e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#### Your Task ####\n","## follow the instructions above\n","\n","import gradio as gr\n","from diffusers import StableDiffusionPipeline\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\"/share/LLMs/stable-diffusion-2-1\", use_auth_token=False)\n","pipe = pipe.to(\"cuda\")\n","gr.Interface.from_pipeline(pipe).launch()"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
