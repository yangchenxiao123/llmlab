{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Lab 8 Supervised Fine Tuning\n","\n","In this lab, we will perform parameter efficient finetuning (PEFT) to finetune a llama-2 model, using the HuggingFace SFTTrainer tool from its trl library.\n"]},{"cell_type":"markdown","metadata":{},"source":["## 0. Dependencies and compatibility"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n","Collecting trl (from -r requirements.txt (line 1))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/97/7e/274ed94ab7da21db4b7cbccad2bf2ed0940082a929b1512e508351b289f5/trl-0.8.6-py3-none-any.whl (245 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hCollecting fire (from -r requirements.txt (line 2))\n","  Using cached fire-0.6.0-py2.py3-none-any.whl\n","Collecting bitsandbytes (from -r requirements.txt (line 3))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/2f/a4/d8c8c1f69ceb3afdc285d62c65bec8d46900d70e81c9a8b24883001e23f8/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (2.1.1)\n","Requirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (4.38.1)\n","Requirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (1.26.0)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (0.27.2)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 1)) (2.17.1)\n","Collecting tyro>=0.5.11 (from trl->-r requirements.txt (line 1))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/11/9d/bad5061876ee331cd2fc23f7a1fdbdb13ea2d65738fbb4e354b3ba644865/tyro-0.8.4-py3-none-any.whl (102 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->-r requirements.txt (line 2)) (1.16.0)\n","Collecting termcolor (from fire->-r requirements.txt (line 2))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.9.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (4.10.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (1.11.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl->-r requirements.txt (line 1)) (2023.10.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (0.21.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl->-r requirements.txt (line 1)) (4.65.0)\n","Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl->-r requirements.txt (line 1))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n","Collecting rich>=11.1.0 (from tyro>=0.5.11->trl->-r requirements.txt (line 1))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl (240 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl->-r requirements.txt (line 1))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/e2/d1/a1d3189e7873408b9dc396aef0d7926c198b0df2aa3ddb5b539d3e89a70f/shtab-1.7.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl->-r requirements.txt (line 1)) (5.9.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (15.0.0)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (2.2.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (0.70.16)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl->-r requirements.txt (line 1)) (3.9.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl->-r requirements.txt (line 1)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl->-r requirements.txt (line 1)) (2023.7.22)\n","Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1)) (2.15.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl->-r requirements.txt (line 1)) (2.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl->-r requirements.txt (line 1)) (2024.1)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl->-r requirements.txt (line 1)) (1.3.0)\n","Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 1))\n","  Downloading https://mirrors.aliyun.com/pypi/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n","Installing collected packages: termcolor, shtab, mdurl, docstring-parser, markdown-it-py, fire, rich, bitsandbytes, tyro, trl\n","Successfully installed bitsandbytes-0.43.1 docstring-parser-0.16 fire-0.6.0 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.1 shtab-1.7.1 termcolor-2.4.0 trl-0.8.6 tyro-0.8.4\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Your GPU supports bfloat16: you can accelerate training by setting \n","          bnb_4bit_compute_dtype to torch.bfloat16 and bf16 in the trainer to True\n"]}],"source":["# Test whether your GPU supports bfloat16\n","import torch\n","major, _ = torch.cuda.get_device_capability()\n","if major >= 8:\n","    print(\"\"\"Your GPU supports bfloat16: you can accelerate training by setting \n","          bnb_4bit_compute_dtype to torch.bfloat16 and bf16 in the trainer to True\"\"\")"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Prepare the dataset\n","\n","See the notebook 01_prepare_data.ipynb\n","\n","Make sure that you generate the correct dataset using that notebook before continuing. "]},{"cell_type":"markdown","metadata":{},"source":["## 2. Get the original inference results (baseline)"]},{"cell_type":"markdown","metadata":{},"source":["Here we perform an inference on the original llama-2 model."]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Setting up the inference (same as previous labs)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["################################################################################\n","# Shared parameters between inference and SFT training\n","################################################################################\n","\n","# The base model\n","model_name = \"/share/model/llama-2-7b-chat-hf\"\n","# Use a single GPU\n","# device_map = {'':0}\n","# Use all GPUs\n","device_map = \"auto\"\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","from transformers import BitsAndBytesConfig\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit= True,    # use 4-bit precision for base model loading\n","    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n","    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n","    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fac1bbaaa7cd42ef94d46cd391eaeab5","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    TrainingArguments,\n","    pipeline,\n",")\n","\n","# Load base model with bnb config\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"left\"\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<s>[INST] Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\n"," def solve(list): [/INST]  Sure! Here is a Python function that takes a list as input and returns the mode of the list:\n","```\n","def solve(list):\n","    # Initialize a dictionary to count the frequency of each element\n","    frequency_dict = {}\n","    for element in list:\n","        # Increment the frequency of the element in the dictionary\n","        frequency_dict[element] = (frequency_dict.get(element, 0) + 1) % 2\n","    # Return the most frequent element or -1 if there are multiple modes\n","    return frequency_dict.most_common(1)[0][1]\n","```\n","Here's how the function works:\n","1. It initializes a dictionary `frequency_dict` to count the frequency of each element in the input list.\n","2. It loops through each element in the list and increments the frequency of the element in the dictionary.\n","3. It uses the modulo operator (`% 2`) to ensure that the frequency of each element is always between 0 and 1, which makes it easier to find the most frequent element.\n","4. It uses the `most_common()` method of the dictionary to find the most frequent element in the list. The `most_common()` method returns a list of tuples, where each tuple contains the most frequent element and its frequency.\n","5. The function returns the most frequent element or -1 if there are multiple modes in the list.\n","Here's an example of how you can use the function:\n","```\n","list = [1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5]\n","# Print the mode of the list\n","print(solve(list)) # Output: 3\n","\n","```\n","In this example, the mode of the list is 3, so the function returns 3. If there were multiple modes in the list (e.g., [1, 2, 3, 3, 3, 4, 4, 4, 5]), the function would return -1.\n"]}],"source":["# Run text generation pipeline with our next model\n","prompt = \"Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\\n def solve(list):\"\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2 Evaluating the code generated"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","def solve(list):\n","    # Initialize a dictionary to count the frequency of each element\n","    frequency_dict = {}\n","    for element in list:\n","        # Increment the frequency of the element in the dictionary\n","        frequency_dict[element] = (frequency_dict.get(element, 0) + 1) % 2\n","    # Return the most frequent element or -1 if there are multiple modes\n","    return frequency_dict.most_common(1)[0][1]\n","\n"]}],"source":["# Using regrex to capture the generated Python to a string\n","import re\n","\n","def extract_first_code_snippet(text):\n","    # Use a regular expression to find the first code snippet enclosed in triple backticks\n","    match = re.search(r\"```(.*?)```\", text, re.S)\n","    if match:\n","        # Return the first matched group, which is the content within the backticks\n","        return match.group(1)\n","    else:\n","        # Return None if no match is found\n","        return None\n","    \n","code = extract_first_code_snippet(result[0]['generated_text'])\n","print(code)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Define a testcase using the standard python unittest library\n","\n","import unittest\n","\n","# place holder for the AI generated code\n","def solve(list):\n","    return 0  \n","\n","class TestGeneratedCode(unittest.TestCase):\n","\n","    def test_no_single_mode(self):\n","        self.assertEqual(solve([3, 2, 1]), -1)\n","\n","    def test_single_mode(self):\n","        self.assertEqual(solve([4, 9, 2, 33, 2]), 2)\n","\n","    def test_no_single_mode_3(self):\n","        self.assertEqual(solve([7, 9, 11, 323, 996]), -1)\n","\n","def run_all_tests():\n","    unittest.main(argv=[''], verbosity=2, exit=False) "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["test_no_single_mode (__main__.TestGeneratedCode) ... ERROR\n","test_no_single_mode_3 (__main__.TestGeneratedCode) ... ERROR\n","test_single_mode (__main__.TestGeneratedCode) ... ERROR\n","\n","======================================================================\n","ERROR: test_no_single_mode (__main__.TestGeneratedCode)\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_954/2030590586.py\", line 12, in test_no_single_mode\n","    self.assertEqual(solve([3, 2, 1]), -1)\n","  File \"<string>\", line 9, in solve\n","AttributeError: 'dict' object has no attribute 'most_common'\n","\n","======================================================================\n","ERROR: test_no_single_mode_3 (__main__.TestGeneratedCode)\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_954/2030590586.py\", line 18, in test_no_single_mode_3\n","    self.assertEqual(solve([7, 9, 11, 323, 996]), -1)\n","  File \"<string>\", line 9, in solve\n","AttributeError: 'dict' object has no attribute 'most_common'\n","\n","======================================================================\n","ERROR: test_single_mode (__main__.TestGeneratedCode)\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_954/2030590586.py\", line 15, in test_single_mode\n","    self.assertEqual(solve([4, 9, 2, 33, 2]), 2)\n","  File \"<string>\", line 9, in solve\n","AttributeError: 'dict' object has no attribute 'most_common'\n","\n","----------------------------------------------------------------------\n","Ran 3 tests in 0.008s\n","\n","FAILED (errors=3)\n"]}],"source":["exec(code)  # run the generated code to redefine solve() function\n","run_all_tests() # Expect to fail"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Finetune the Llama2 model\n","In this section, we will finetune a chat model. We choose Llama-chat-7B as our base model and our dataset is code instrcutions, which guides LLM to write programming code from given natural language. Don't panic since this is not a difficult task and we will separate this task into several procedures."]},{"cell_type":"markdown","metadata":{},"source":["### 3.1  Set training arguments\n","In this subsection, you need to read the given code snippets below. If you have some questions, you can either refer to the official documents or discuss with TAs or you classmates."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["################################################################################\n","# Model name and directories\n","################################################################################\n","\n","# The base model\n","model_name = \"/share/model/llama-2-7b-chat-hf\"\n","# The instruction dataset to use\n","dataset_name = \"/scratch2/py18k\"\n","# Fine-tuned model name\n","new_model = \"/scratch2/llama-2-7b-py18k\"\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"/scratch2/results\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","bias=\"none\"\n","task_type=\"CAUSAL_LM\"\n","\n","################################################################################\n","# Training parameters (passed to TrainingArguments)\n","################################################################################\n","\n","# Number of training epochs\n","num_train_epochs = 1\n","# Number of training steps (overrides num_train_epochs)\n","# max_steps = 100\n","# Enable fp16/bf16 training (set bf16 to True if supported by your GPU)\n","fp16 = False\n","bf16 = True\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","################################################################################\n","# Monitoring parameters\n","################################################################################\n","\n","# Logging dir (for tensorboard)\n","logging_dir = f\"{output_dir}/logs\"\n","# Log every X updates steps\n","logging_steps = 25\n","# Monitoring and Visualizing tools\n","report_to = \"tensorboard\"\n","\n","################################################################################\n","# SFTTrainer parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = 512\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2 Load Training Dataset (prepared in section 1)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': \"<s>[INST] Create a for loop in Python that prints the output of a multiplication table for numbers from 1 to 10. The given input is -. [/INST] ```for i in range(1, 11):\\n  for j in range(1, 11):\\n    print(i * j, end='\\\\t')\\n  print()``` </s>\"}\n"]}],"source":["# Load dataset\n","from datasets import load_from_disk\n","dataset = load_from_disk(dataset_name)\n","print(dataset[0])"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3 Finetuning the model"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3.1 Construct the configuration objects"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from peft import PeftModel\n","from trl import SFTTrainer\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    #max_steps=max_steps,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    logging_steps=logging_steps,\n","    logging_dir=logging_dir,\n","    report_to=report_to, \n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from peft import LoraConfig\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=bias,\n","    task_type=task_type,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3.2 Initialize SFTTrainer"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8247474fb104483f84d66df9a7b65b42","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/18612 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n"]}],"source":["# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3.3 start the tensorboard for monitoring"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["Launching TensorBoard..."]},"metadata":{},"output_type":"display_data"}],"source":["import tensorboard\n","\n","# Visualize the finetuning process.\n","#%load_ext tensorboard\n","%reload_ext tensorboard\n","%tensorboard --logdir /scratch2/results/logs"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3.4 Train the model (takes about half an hour on 4090)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# torch.cuda.reset_peak_memory_stats()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4653' max='4653' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4653/4653 42:35, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.830900</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.287700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.012500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.954200</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.767500</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.746800</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.661400</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.729200</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>0.753800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.695300</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>0.714500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.719100</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>0.720000</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.699800</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>0.764500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.685500</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>0.707400</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.704300</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>0.686100</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.711900</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>0.694900</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.686200</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>0.671900</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.722200</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>0.697000</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.707800</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>0.684800</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.704800</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>0.712600</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.642200</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>0.696900</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.702700</td>\n","    </tr>\n","    <tr>\n","      <td>825</td>\n","      <td>0.687800</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.704000</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>0.682100</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.739800</td>\n","    </tr>\n","    <tr>\n","      <td>925</td>\n","      <td>0.613600</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.706900</td>\n","    </tr>\n","    <tr>\n","      <td>975</td>\n","      <td>0.681800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>1025</td>\n","      <td>0.699100</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.690000</td>\n","    </tr>\n","    <tr>\n","      <td>1075</td>\n","      <td>0.684100</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.678600</td>\n","    </tr>\n","    <tr>\n","      <td>1125</td>\n","      <td>0.665300</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.690800</td>\n","    </tr>\n","    <tr>\n","      <td>1175</td>\n","      <td>0.663800</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.652700</td>\n","    </tr>\n","    <tr>\n","      <td>1225</td>\n","      <td>0.667000</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.655100</td>\n","    </tr>\n","    <tr>\n","      <td>1275</td>\n","      <td>0.667100</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.672700</td>\n","    </tr>\n","    <tr>\n","      <td>1325</td>\n","      <td>0.652800</td>\n","    </tr>\n","    <tr>\n","      <td>1350</td>\n","      <td>0.649500</td>\n","    </tr>\n","    <tr>\n","      <td>1375</td>\n","      <td>0.637300</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.679400</td>\n","    </tr>\n","    <tr>\n","      <td>1425</td>\n","      <td>0.653500</td>\n","    </tr>\n","    <tr>\n","      <td>1450</td>\n","      <td>0.733200</td>\n","    </tr>\n","    <tr>\n","      <td>1475</td>\n","      <td>0.682800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.647500</td>\n","    </tr>\n","    <tr>\n","      <td>1525</td>\n","      <td>0.662900</td>\n","    </tr>\n","    <tr>\n","      <td>1550</td>\n","      <td>0.645800</td>\n","    </tr>\n","    <tr>\n","      <td>1575</td>\n","      <td>0.666100</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.663400</td>\n","    </tr>\n","    <tr>\n","      <td>1625</td>\n","      <td>0.675100</td>\n","    </tr>\n","    <tr>\n","      <td>1650</td>\n","      <td>0.645100</td>\n","    </tr>\n","    <tr>\n","      <td>1675</td>\n","      <td>0.674600</td>\n","    </tr>\n","    <tr>\n","      <td>1700</td>\n","      <td>0.646900</td>\n","    </tr>\n","    <tr>\n","      <td>1725</td>\n","      <td>0.685900</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.722300</td>\n","    </tr>\n","    <tr>\n","      <td>1775</td>\n","      <td>0.649700</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.689000</td>\n","    </tr>\n","    <tr>\n","      <td>1825</td>\n","      <td>0.666500</td>\n","    </tr>\n","    <tr>\n","      <td>1850</td>\n","      <td>0.660700</td>\n","    </tr>\n","    <tr>\n","      <td>1875</td>\n","      <td>0.657900</td>\n","    </tr>\n","    <tr>\n","      <td>1900</td>\n","      <td>0.706300</td>\n","    </tr>\n","    <tr>\n","      <td>1925</td>\n","      <td>0.676400</td>\n","    </tr>\n","    <tr>\n","      <td>1950</td>\n","      <td>0.704200</td>\n","    </tr>\n","    <tr>\n","      <td>1975</td>\n","      <td>0.648100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.673500</td>\n","    </tr>\n","    <tr>\n","      <td>2025</td>\n","      <td>0.658300</td>\n","    </tr>\n","    <tr>\n","      <td>2050</td>\n","      <td>0.637500</td>\n","    </tr>\n","    <tr>\n","      <td>2075</td>\n","      <td>0.630000</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>0.640200</td>\n","    </tr>\n","    <tr>\n","      <td>2125</td>\n","      <td>0.668700</td>\n","    </tr>\n","    <tr>\n","      <td>2150</td>\n","      <td>0.661800</td>\n","    </tr>\n","    <tr>\n","      <td>2175</td>\n","      <td>0.641900</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.689100</td>\n","    </tr>\n","    <tr>\n","      <td>2225</td>\n","      <td>0.642300</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.642500</td>\n","    </tr>\n","    <tr>\n","      <td>2275</td>\n","      <td>0.714700</td>\n","    </tr>\n","    <tr>\n","      <td>2300</td>\n","      <td>0.635900</td>\n","    </tr>\n","    <tr>\n","      <td>2325</td>\n","      <td>0.668500</td>\n","    </tr>\n","    <tr>\n","      <td>2350</td>\n","      <td>0.665800</td>\n","    </tr>\n","    <tr>\n","      <td>2375</td>\n","      <td>0.690800</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.645900</td>\n","    </tr>\n","    <tr>\n","      <td>2425</td>\n","      <td>0.666800</td>\n","    </tr>\n","    <tr>\n","      <td>2450</td>\n","      <td>0.656100</td>\n","    </tr>\n","    <tr>\n","      <td>2475</td>\n","      <td>0.623400</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.689000</td>\n","    </tr>\n","    <tr>\n","      <td>2525</td>\n","      <td>0.612600</td>\n","    </tr>\n","    <tr>\n","      <td>2550</td>\n","      <td>0.641300</td>\n","    </tr>\n","    <tr>\n","      <td>2575</td>\n","      <td>0.673000</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.638900</td>\n","    </tr>\n","    <tr>\n","      <td>2625</td>\n","      <td>0.629900</td>\n","    </tr>\n","    <tr>\n","      <td>2650</td>\n","      <td>0.588300</td>\n","    </tr>\n","    <tr>\n","      <td>2675</td>\n","      <td>0.630800</td>\n","    </tr>\n","    <tr>\n","      <td>2700</td>\n","      <td>0.636800</td>\n","    </tr>\n","    <tr>\n","      <td>2725</td>\n","      <td>0.622000</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.671700</td>\n","    </tr>\n","    <tr>\n","      <td>2775</td>\n","      <td>0.646200</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.634300</td>\n","    </tr>\n","    <tr>\n","      <td>2825</td>\n","      <td>0.636500</td>\n","    </tr>\n","    <tr>\n","      <td>2850</td>\n","      <td>0.626300</td>\n","    </tr>\n","    <tr>\n","      <td>2875</td>\n","      <td>0.613900</td>\n","    </tr>\n","    <tr>\n","      <td>2900</td>\n","      <td>0.657100</td>\n","    </tr>\n","    <tr>\n","      <td>2925</td>\n","      <td>0.642800</td>\n","    </tr>\n","    <tr>\n","      <td>2950</td>\n","      <td>0.646000</td>\n","    </tr>\n","    <tr>\n","      <td>2975</td>\n","      <td>0.632800</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.648300</td>\n","    </tr>\n","    <tr>\n","      <td>3025</td>\n","      <td>0.681400</td>\n","    </tr>\n","    <tr>\n","      <td>3050</td>\n","      <td>0.618900</td>\n","    </tr>\n","    <tr>\n","      <td>3075</td>\n","      <td>0.647800</td>\n","    </tr>\n","    <tr>\n","      <td>3100</td>\n","      <td>0.622300</td>\n","    </tr>\n","    <tr>\n","      <td>3125</td>\n","      <td>0.649200</td>\n","    </tr>\n","    <tr>\n","      <td>3150</td>\n","      <td>0.645100</td>\n","    </tr>\n","    <tr>\n","      <td>3175</td>\n","      <td>0.644700</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.648600</td>\n","    </tr>\n","    <tr>\n","      <td>3225</td>\n","      <td>0.630800</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.662400</td>\n","    </tr>\n","    <tr>\n","      <td>3275</td>\n","      <td>0.633700</td>\n","    </tr>\n","    <tr>\n","      <td>3300</td>\n","      <td>0.655900</td>\n","    </tr>\n","    <tr>\n","      <td>3325</td>\n","      <td>0.653700</td>\n","    </tr>\n","    <tr>\n","      <td>3350</td>\n","      <td>0.653400</td>\n","    </tr>\n","    <tr>\n","      <td>3375</td>\n","      <td>0.628500</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.627900</td>\n","    </tr>\n","    <tr>\n","      <td>3425</td>\n","      <td>0.680200</td>\n","    </tr>\n","    <tr>\n","      <td>3450</td>\n","      <td>0.639100</td>\n","    </tr>\n","    <tr>\n","      <td>3475</td>\n","      <td>0.673200</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.648100</td>\n","    </tr>\n","    <tr>\n","      <td>3525</td>\n","      <td>0.639300</td>\n","    </tr>\n","    <tr>\n","      <td>3550</td>\n","      <td>0.657800</td>\n","    </tr>\n","    <tr>\n","      <td>3575</td>\n","      <td>0.612300</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.605700</td>\n","    </tr>\n","    <tr>\n","      <td>3625</td>\n","      <td>0.611500</td>\n","    </tr>\n","    <tr>\n","      <td>3650</td>\n","      <td>0.656600</td>\n","    </tr>\n","    <tr>\n","      <td>3675</td>\n","      <td>0.649600</td>\n","    </tr>\n","    <tr>\n","      <td>3700</td>\n","      <td>0.618100</td>\n","    </tr>\n","    <tr>\n","      <td>3725</td>\n","      <td>0.621600</td>\n","    </tr>\n","    <tr>\n","      <td>3750</td>\n","      <td>0.704300</td>\n","    </tr>\n","    <tr>\n","      <td>3775</td>\n","      <td>0.651800</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.633600</td>\n","    </tr>\n","    <tr>\n","      <td>3825</td>\n","      <td>0.643900</td>\n","    </tr>\n","    <tr>\n","      <td>3850</td>\n","      <td>0.655200</td>\n","    </tr>\n","    <tr>\n","      <td>3875</td>\n","      <td>0.603300</td>\n","    </tr>\n","    <tr>\n","      <td>3900</td>\n","      <td>0.633500</td>\n","    </tr>\n","    <tr>\n","      <td>3925</td>\n","      <td>0.671700</td>\n","    </tr>\n","    <tr>\n","      <td>3950</td>\n","      <td>0.656100</td>\n","    </tr>\n","    <tr>\n","      <td>3975</td>\n","      <td>0.647300</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.627700</td>\n","    </tr>\n","    <tr>\n","      <td>4025</td>\n","      <td>0.667800</td>\n","    </tr>\n","    <tr>\n","      <td>4050</td>\n","      <td>0.642700</td>\n","    </tr>\n","    <tr>\n","      <td>4075</td>\n","      <td>0.622300</td>\n","    </tr>\n","    <tr>\n","      <td>4100</td>\n","      <td>0.639700</td>\n","    </tr>\n","    <tr>\n","      <td>4125</td>\n","      <td>0.626400</td>\n","    </tr>\n","    <tr>\n","      <td>4150</td>\n","      <td>0.616100</td>\n","    </tr>\n","    <tr>\n","      <td>4175</td>\n","      <td>0.655300</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.652200</td>\n","    </tr>\n","    <tr>\n","      <td>4225</td>\n","      <td>0.663800</td>\n","    </tr>\n","    <tr>\n","      <td>4250</td>\n","      <td>0.640900</td>\n","    </tr>\n","    <tr>\n","      <td>4275</td>\n","      <td>0.637000</td>\n","    </tr>\n","    <tr>\n","      <td>4300</td>\n","      <td>0.651300</td>\n","    </tr>\n","    <tr>\n","      <td>4325</td>\n","      <td>0.639700</td>\n","    </tr>\n","    <tr>\n","      <td>4350</td>\n","      <td>0.651200</td>\n","    </tr>\n","    <tr>\n","      <td>4375</td>\n","      <td>0.672900</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.661500</td>\n","    </tr>\n","    <tr>\n","      <td>4425</td>\n","      <td>0.617600</td>\n","    </tr>\n","    <tr>\n","      <td>4450</td>\n","      <td>0.660300</td>\n","    </tr>\n","    <tr>\n","      <td>4475</td>\n","      <td>0.630800</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.652000</td>\n","    </tr>\n","    <tr>\n","      <td>4525</td>\n","      <td>0.650100</td>\n","    </tr>\n","    <tr>\n","      <td>4550</td>\n","      <td>0.646300</td>\n","    </tr>\n","    <tr>\n","      <td>4575</td>\n","      <td>0.635700</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.648200</td>\n","    </tr>\n","    <tr>\n","      <td>4625</td>\n","      <td>0.586200</td>\n","    </tr>\n","    <tr>\n","      <td>4650</td>\n","      <td>0.663500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /share/model/llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n","  warnings.warn(\n"]}],"source":["# # Train model\n","trainer.train()\n","\n","# Save trained model\n","trainer.model.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# release GPU memory here\n","\n","torch.cuda.reset_peak_memory_stats()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["### 3.3.5 Merge Lora_model with Base model and save the merged model"]},{"cell_type":"markdown","metadata":{},"source":["You should first export Lora model with base model and convert them into hf checkpoint. \n","This makes up the final trained merged model."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09263741ce3c4a98b8199435a2f05407","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","merged_model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"left\"\n","\n","# Save merged model to disk (optional)\n","merged_model.save_pretrained(f'{new_model}_merged')\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# (Optional) Push Lora model to Huggingface hub (optional)\n","# !huggingface-cli login\n","\n","# merged_model.push_to_hub(new_model, use_temp_dir=False)\n","# tokenizer.push_to_hub(new_model, use_temp_dir=False)"]},{"cell_type":"markdown","metadata":{},"source":["### 3.6 examine the results in tensorboard\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# set up port forwarding in vscode\n","# open the tensorboard page at http://localhost:6006"]},{"cell_type":"markdown","metadata":{},"source":["### 3.7 Test the SFTed model"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["<s>[INST] Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\n"," def solve(list):\n"," [/INST] ```def solve(list):\n","    # initialize the count of each element\n","    counts = {}\n","    for element in list:\n","        if element in counts:\n","            counts[element] += 1\n","        else:\n","            counts[element] = 1\n","    # calculate the mode\n","    mode = None\n","    max_count = 0\n","    for element in counts:\n","        if counts[element] > max_count:\n","            max_count = counts[element]\n","            mode = element\n","    # return the mode\n","    return mode\n","\n","# test\n","list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","print(solve(list))```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. [/INST] ```# test\n","list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","print(solve(list))```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].```  •\tThe output is 5.```  •\tThe given input is list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10].``` \n"]}],"source":["# Run text generation pipeline with our next model\n","prompt = \"Write a Python function to return the mode (the value or values that appear most frequently within that list) in a given list. If there are multiple modes, return -1. You should generate a function:\\n def solve(list):\\n\"\n","pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer, max_length=2048)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["def solve(list):\n","    # initialize the count of each element\n","    counts = {}\n","    for element in list:\n","        if element in counts:\n","            counts[element] += 1\n","        else:\n","            counts[element] = 1\n","    # calculate the mode\n","    mode = None\n","    max_count = 0\n","    for element in counts:\n","        if counts[element] > max_count:\n","            max_count = counts[element]\n","            mode = element\n","    # return the mode\n","    return mode\n","\n","# test\n","list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","print(solve(list))\n"]}],"source":["code = extract_first_code_snippet(result[0]['generated_text'])\n","print(code)\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["test_no_single_mode (__main__.TestGeneratedCode) ... FAIL\n","test_no_single_mode_3 (__main__.TestGeneratedCode) ... FAIL\n","test_single_mode (__main__.TestGeneratedCode) ... ok\n","\n","======================================================================\n","FAIL: test_no_single_mode (__main__.TestGeneratedCode)\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_954/2030590586.py\", line 12, in test_no_single_mode\n","    self.assertEqual(solve([3, 2, 1]), -1)\n","AssertionError: 3 != -1\n","\n","======================================================================\n","FAIL: test_no_single_mode_3 (__main__.TestGeneratedCode)\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"/tmp/ipykernel_954/2030590586.py\", line 18, in test_no_single_mode_3\n","    self.assertEqual(solve([7, 9, 11, 323, 996]), -1)\n","AssertionError: 7 != -1\n","\n","----------------------------------------------------------------------\n","Ran 3 tests in 0.006s\n","\n","FAILED (failures=2)\n"]},{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["exec(code)\n","run_all_tests() # Expect to pass"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
